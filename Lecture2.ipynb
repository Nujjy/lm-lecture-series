{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38953f41-f147-488c-9364-3d106cebd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6d5f1-07bf-4f70-ba76-2819bcff77b0",
   "metadata": {},
   "source": [
    "# Bigram Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc60e0-3c4a-4e6d-9e8d-3f8dab3bbd9c",
   "metadata": {},
   "source": [
    "In the second part of this lecture series, we will demonstrate how to compute the bigram counts using a neural network in PyTorch and how backpropagation can derive the same maximum likelihood estimates.\n",
    "\n",
    "Let's start from the problem of learning a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ that minimizes the loss between the ground truth labels $y$ and the predicted probabilities $\\hat{y}$, where $y, \\hat{y} \\in \\mathbb{R}^n$. E.g.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &= f\\begin{pmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    x_{0} \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{n} \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\end{pmatrix} = \n",
    "    \\begin{bmatrix}\n",
    "        \\hat{y}_{0} \\\\      \n",
    "        \\hat{y}_{1} \\\\      \n",
    "        \\vdots \\\\\n",
    "        \\hat{y}_{n}\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The cross-entropy loss is commonly used as a measure of the similarity between the predicted values $\\hat{y}$ and the actual values $y$. This is because it is well-suited for classification problems, where the goal is to predict a categorical label for each input instance.\n",
    "\n",
    "$$\n",
    "H(y, \\hat{y}) = -\\sum_{i=1}^n y_i \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "The cross-entropy loss measures the difference between the predicted probability distribution (as represented by $\\hat{y}$) and the true probability distribution (as represented by $y$). It does this by computing the logarithmic loss of the predicted probabilities for the true labels. The resulting value penalizes the model more heavily for predictions that are very wrong (i.e., assigning a high probability to the wrong label) and less heavily for predictions that are somewhat wrong (i.e., assigning a low probability to the correct label).\n",
    "\n",
    "*For example, suppose we have*:\n",
    "\n",
    "\\begin{align*}\n",
    "y &=\n",
    "    \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0\n",
    "    \\end{bmatrix}\n",
    "    \\quad \\text{and} \\quad\n",
    "    \\hat{y} = \n",
    "    \\begin{bmatrix}\n",
    "    0.1 \\\\      \n",
    "    0.2 \\\\      \n",
    "    0.3 \\\\\n",
    "    0.3 \\\\\n",
    "    0.1\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\hat{y}$ represents the predicted probability distribution, which assigns the highest probability to the third label. $y$ represents the true probability distribution, which assigns the highest probability to the fourth label. The cross-entropy loss between $\\hat{y}$ and $y$ is then computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "H(y, \\hat{y}) &= -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i) \\\\\n",
    "&= -(0\\cdot0.1 + 0 \\cdot 0.2 + 1 \\cdot \\log(0.3) + 0 \\cdot \\log(0.3) + 0 \\cdot \\log(0.1)) \\\\\n",
    "&= -( 1 \\cdot \\log(0.3)) \\\\\n",
    "&= 1.203\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now, let's consider a dataset of $T$ training examples \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{ (x_1, y_1), (x_2, y_2), \\dots, (x_T, y_T) \\}\n",
    "$$\n",
    "\n",
    "The goal is to learn a function $f$ that minimizes the average cross-entropy loss between the ground truth labels and the predicted probabilities over the dataset:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{j=1}^T H(y^{(j)}, \\hat{y}^{(j)}) = -\\frac{1}{T} \\sum_{j=1}^T \\log \\hat{y}_c^{(j)} = -\\frac{1}{T} \\sum_{j=1}^T \\log P(c_j|x_j)\n",
    "$$\n",
    "\n",
    "$\\hat{y}_c$ represents the predicted probability of the true class label $c$, given the input $x_j$. In other words, $\\hat{y}_c = P(c|x_j)$, where $c$ is the true class label for the $j$-th example $(x_j, c_j)$ in the dataset.\n",
    "\n",
    "Notice that this expression is the ***average negative log-likelihood (NLL)*** of the true classes given the input data, under the estimated conditional probability distribution $P(c|x)$ obtained from our learned function $f$. By minimizing this average cross-entropy loss, we are effectively maximizing the likelihood of the true classes given the input data, according to the estimated probability distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4ab6e-4a17-42f0-9d88-54d0d2c5ab33",
   "metadata": {},
   "source": [
    "To arrive at this probability we can apply a suitable function to the output of the learned function $f$. The function is responsible for transforming the raw output values into a probability distribution, so that the probabilities of all classes sum to 1.\n",
    "\n",
    "1. **Softmax function**: Given the raw output values, or logits, of the function $f$ for an input $x$, the softmax function calculates the probability of each class $c$ as follows:\n",
    "\n",
    "$$\n",
    "P(c|x) = \\frac{e^{f_c(x)}}{\\sum_{i=1}^n e^{f_i(x)}}\n",
    "$$\n",
    "\n",
    "Here, $f_c(x)$ is the output value corresponding to class $c$, and $n$ is the total number of classes. The softmax function ensures that the probabilities sum to 1, and the output can be interpreted as a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebbb0e-e537-4475-bd74-cc91ea85d782",
   "metadata": {},
   "source": [
    "## Using Neural Networks to determine the Maximum Likilihood Estimates of a Bigram Language Model.\n",
    "\n",
    "Recall from our definition of the likelihood function for a N-Gram language model the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = \\prod_{i=1}^{T} P(c_i | c_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "In this case (i.e. the bigram lanuage model model) the likelihood function calculates the probability of observing a word $c_i$ given its immediate predecessor $c_{i-1}$ and the model parameters $\\theta$.\n",
    "\n",
    "Rather than using the observed frequencies of bigram to estimate the conditional probability distribution $\\theta^{MLE}$. We can instead use a simple feedforward network with an input layer for the previous word, a single hidden layer for our parameters $\\theta$, and an output layer representing the probability distribution over the vocabulary for the next word.\n",
    "\n",
    "As before, we can rewrite the likelihood function as a loss function that we want to minimize during training. The negative log-likelihood (NLL) for the bigram model would be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = - \\sum_{i=1}^{T} \\log P(c_i | c_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "To train the neural network-based bigram model, we will use optimization algorithms like stochastic gradient descent (SGD) or its variants to iteratively update the parameters $\\theta$ to minimize the loss function. This will involve computing the gradients of the loss function with respect to the parameters and updating them accordingly.\n",
    "\n",
    "By minimizing the NLL, we are effectively maximizing the likelihood of our bigram model, finding the best set of parameters (weights) for the neural network. The trained neural network can then be used to predict the next word in a sequence given the previous word, based on the learned bigram probabilities.\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "Suppose we have a neural network $f_{\\theta}: \\mathbb{R}^V \\rightarrow \\mathbb{R}^V$ with learnable parameters $\\theta$. The input to the network is the one-hot vector of the previous character, and the output is the predicted probability distribution of the next character.\n",
    "\n",
    "Let's represent the text data using the character set $\\mathcal{C} = \\{c_1, c_2, \\dots, c_M\\}$, where $M$ is the number of unique characters in the text. The goal is to learn the conditional probability $P(c_i | c_j)$ for all pairs of characters $c_i, c_j \\in C$.\n",
    "\n",
    "Define a function $\\phi: C \\rightarrow \\mathbb{R}^M$ to represent the one-hot encoding of characters:\n",
    "\n",
    "$$\n",
    "\\phi(c_i) = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^M,\n",
    "$$\n",
    "\n",
    "where the $1$ is at the $i$-th position.\n",
    "\n",
    "\n",
    "Let $x$ denote the input to the neural network and let \\mathbf{W} denote the weight matrix layer (i.e. parameters), respectively. We can express the forward pass through the network as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\phi(c_j),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \\mathbf{W} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\text{softmax}(\\mathbf{h}),\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly let's define the softmax activation which maps \n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_{k=1}^{V} \\exp(z_k)}\n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\mathcal{D} = \\{(c_{i-1}, c_i) |$ for $i \\in 1 \\dots T\\}$ be the set of all pairs of consecutive characters in the text, where $T$ is the total number of training examples. We want to learn the parameters $\\theta$ that minimize the average cross-entropy loss over the dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{T - 1} \\sum_{(c_j, c_i) \\in \\mathcal{D}} \\log \\hat{y}_i,\n",
    "$$\n",
    "\n",
    "where $\\hat{y} = f_{\\theta}(\\phi(c_j))$ is the predicted probability distribution of the next character given the previous character.\n",
    "\n",
    "\n",
    "Once the neural network is trained, we can estimate the conditional probability $P(c_i | c_j)$ for any pair of characters as follows:\n",
    "\n",
    "$$\n",
    "P(c_i | c_j) \\approx \\hat{y}_i = \\text{softmax}(\\mathbf{W} \\cdot \\mathbf{x}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcf4fde-1cdd-4e31-b255-74b3fcad2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utilities import read_file #helper function for reading in files.\n",
    "\n",
    "names = list(read_file('names.txt'))\n",
    "C = sorted(list(set(''.join(names))))\n",
    "ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "ctoi['.'] = 0\n",
    "itos = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for n in names[:1]:\n",
    "    n = ['.'] + list(n) +['.']\n",
    "    for c1, c2 in zip(n, n[1:]): \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad42a88-529e-4b18-953b-87c0aaa54ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "W = torch.rand((27,27))\n",
    "X = torch.tensor(xs)\n",
    "Y = torch.tensor(ys)\n",
    "\n",
    "X = F.one_hot(torch.tensor(xs), num_classes=27).float()\n",
    "logits = X @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(X)), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ce3054-4bb0-4319-97eb-0660047b71b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0359, 0.0312, 0.0482, 0.0578, 0.0330, 0.0247, 0.0317, 0.0310, 0.0249,\n",
       "         0.0270, 0.0333, 0.0459, 0.0282, 0.0432, 0.0510, 0.0610, 0.0273, 0.0532,\n",
       "         0.0330, 0.0300, 0.0305, 0.0305, 0.0432, 0.0232, 0.0237, 0.0539, 0.0433],\n",
       "        [0.0222, 0.0269, 0.0496, 0.0267, 0.0372, 0.0289, 0.0498, 0.0430, 0.0400,\n",
       "         0.0246, 0.0308, 0.0238, 0.0297, 0.0446, 0.0463, 0.0296, 0.0354, 0.0337,\n",
       "         0.0354, 0.0269, 0.0437, 0.0514, 0.0568, 0.0255, 0.0433, 0.0519, 0.0424],\n",
       "        [0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
       "         0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
       "         0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284],\n",
       "        [0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
       "         0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
       "         0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284],\n",
       "        [0.0399, 0.0394, 0.0242, 0.0420, 0.0428, 0.0278, 0.0252, 0.0320, 0.0231,\n",
       "         0.0323, 0.0317, 0.0287, 0.0404, 0.0604, 0.0510, 0.0330, 0.0491, 0.0280,\n",
       "         0.0236, 0.0234, 0.0350, 0.0263, 0.0303, 0.0581, 0.0464, 0.0512, 0.0547]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53779370-7f2d-46b3-afc8-f79342d94d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0359, 0.0312, 0.0482, 0.0578, 0.0330, 0.0247, 0.0317, 0.0310, 0.0249,\n",
       "         0.0270, 0.0333, 0.0459, 0.0282, 0.0432, 0.0510, 0.0610, 0.0273, 0.0532,\n",
       "         0.0330, 0.0300, 0.0305, 0.0305, 0.0432, 0.0232, 0.0237, 0.0539, 0.0433],\n",
       "        [0.0222, 0.0269, 0.0496, 0.0267, 0.0372, 0.0289, 0.0498, 0.0430, 0.0400,\n",
       "         0.0246, 0.0308, 0.0238, 0.0297, 0.0446, 0.0463, 0.0296, 0.0354, 0.0337,\n",
       "         0.0354, 0.0269, 0.0437, 0.0514, 0.0568, 0.0255, 0.0433, 0.0519, 0.0424],\n",
       "        [0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
       "         0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
       "         0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284],\n",
       "        [0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
       "         0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
       "         0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284],\n",
       "        [0.0399, 0.0394, 0.0242, 0.0420, 0.0428, 0.0278, 0.0252, 0.0320, 0.0231,\n",
       "         0.0323, 0.0317, 0.0287, 0.0404, 0.0604, 0.0510, 0.0330, 0.0491, 0.0280,\n",
       "         0.0236, 0.0234, 0.0350, 0.0263, 0.0303, 0.0581, 0.0464, 0.0512, 0.0547]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ff65e6-7aa5-4524-8d75-bd5d1f8b2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0359, 0.0312, 0.0482, 0.0578, 0.0330, 0.0247, 0.0317, 0.0310, 0.0249,\n",
      "        0.0270, 0.0333, 0.0459, 0.0282, 0.0432, 0.0510, 0.0610, 0.0273, 0.0532,\n",
      "        0.0330, 0.0300, 0.0305, 0.0305, 0.0432, 0.0232, 0.0237, 0.0539, 0.0433])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.024730652570724487\n",
      "log likelihood: -3.699711799621582\n",
      "negative log likelihood: 3.699711799621582\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0222, 0.0269, 0.0496, 0.0267, 0.0372, 0.0289, 0.0498, 0.0430, 0.0400,\n",
      "        0.0246, 0.0308, 0.0238, 0.0297, 0.0446, 0.0463, 0.0296, 0.0354, 0.0337,\n",
      "        0.0354, 0.0269, 0.0437, 0.0514, 0.0568, 0.0255, 0.0433, 0.0519, 0.0424])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.044582758098840714\n",
      "log likelihood: -3.110408067703247\n",
      "negative log likelihood: 3.110408067703247\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
      "        0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
      "        0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.023251745849847794\n",
      "log likelihood: -3.7613749504089355\n",
      "negative log likelihood: 3.7613749504089355\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0546, 0.0345, 0.0292, 0.0246, 0.0333, 0.0537, 0.0605, 0.0256, 0.0336,\n",
      "        0.0278, 0.0586, 0.0374, 0.0438, 0.0233, 0.0301, 0.0325, 0.0493, 0.0311,\n",
      "        0.0300, 0.0295, 0.0610, 0.0337, 0.0236, 0.0423, 0.0404, 0.0276, 0.0284])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.03454810753464699\n",
      "log likelihood: -3.3654024600982666\n",
      "negative log likelihood: 3.3654024600982666\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0399, 0.0394, 0.0242, 0.0420, 0.0428, 0.0278, 0.0252, 0.0320, 0.0231,\n",
      "        0.0323, 0.0317, 0.0287, 0.0404, 0.0604, 0.0510, 0.0330, 0.0491, 0.0280,\n",
      "        0.0236, 0.0234, 0.0350, 0.0263, 0.0303, 0.0581, 0.0464, 0.0512, 0.0547])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.03989464044570923\n",
      "log likelihood: -3.221513271331787\n",
      "negative log likelihood: 3.221513271331787\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.4316821098327637\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    # i-th bigram:\n",
    "    x = xs[i]\n",
    "    y = ys[i]\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ff62de-8caf-4ac4-8ea2-849d4cc87615",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d2c0e5-d668-495d-a06d-8d8ffc37f43b",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a commonly used optimization algorithm in machine learning and other fields. The goal of gradient descent is to find the minimum of a function by iteratively adjusting the parameters of the function in the direction of the negative gradient.\n",
    "\n",
    "In machine learning, the function being optimized is typically a loss function $\\mathcal{L}$ that measures the error between the predicted and actual values. Therefore, the update rule for the parameters W of a model using gradient descent can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W_{t+1}} = \\mathbf{W_t} - \\alpha \\frac{\\partial \\mathcal{L}(\\mathbf{W_t})}{\\partial \\mathbf{W_t}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{W_t}$ represents the current value of the parameters at iteration $t$, $\\alpha$ is the learning rate which determines the step size at each iteration, and $\\frac{\\partial \\mathcal{L}((\\mathbf{W_t})}{\\partial \\mathbf{W_t}} $ is the partial derivative of the loss function with respect to the parameters evaluated at the current parameter values.\n",
    "\n",
    "The algorithm proceeds by iteratively updating the parameters until convergence or a stopping criterion is met. The gradient descent algorithm is often used in machine learning to optimize the model parameters by minimizing a loss function with respect to the parameters W.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138df8a6-c15e-4ca7-8285-eead48b53e81",
   "metadata": {},
   "source": [
    "## A quick example\n",
    "\n",
    "Let's perform gradient descene on $y = x^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8ed08f-d4bd-452d-a6ed-6d5b0bd8f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2      # Define y = x^2\n",
    "df = lambda x: 2*x      # Define dy/dx = 2x\n",
    "\n",
    "x = 1                   # Define an initial starting poing\n",
    "alpha = 0.000001        # Define a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d79010-ad7f-46fc-94a3-9265a0f8a6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008816639999999997"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = f(x) - 0.1*df(x)\n",
    "x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0de7bb17-4b28-449d-a681-dbe6daebe0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utilities import plot_gradient_descent\n",
    "x_start = 3\n",
    "learning_rate = 0.1\n",
    "iterations = 50\n",
    "\n",
    "#f = lambda x: x**4 - 8*x**2 + 3*x + 10\n",
    "#df = lambda x: 4*x**3 - 16*x + 3\n",
    "\n",
    "plot_gradient_descent(f, df, x_start, learning_rate, iterations, start=-3, end=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08fa5ce8-b9bf-44b6-acbf-e214f0454937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Computation Graphs in Pytorch (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "e7604d96-130d-48e1-983b-93339f26669a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.4 (20230421.1958)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"263pt\" height=\"338pt\"\n",
       " viewBox=\"0.00 0.00 263.00 338.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 334)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-334 259,-334 259,4 -4,4\"/>\n",
       "<!-- 6440611536 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6440611536</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"89,-329.62 35,-329.62 35,-299.12 89,-299.12 89,-329.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-316.12\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-304.88\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 6436742304 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>6436742304</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"112,-262.75 12,-262.75 12,-243.5 112,-243.5 112,-262.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-249.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6440611536&#45;&gt;6436742304 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6440611536&#45;&gt;6436742304</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62,-298.73C62,-291.18 62,-281.86 62,-273.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.5,-273.91 62,-263.91 58.5,-273.91 65.5,-273.91\"/>\n",
       "</g>\n",
       "<!-- 6440611136 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6440611136</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"255,-152.25 197,-152.25 197,-121.75 255,-121.75 255,-152.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-138.75\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-127.5\" font-family=\"monospace\" font-size=\"10.00\"> (3, 1)</text>\n",
       "</g>\n",
       "<!-- 6436742784 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6436742784</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"236,-207.5 142,-207.5 142,-188.25 236,-188.25 236,-207.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\">ViewBackward0</text>\n",
       "</g>\n",
       "<!-- 6436742784&#45;&gt;6440611136 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6436742784&#45;&gt;6440611136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.62,-187.93C199,-180.96 205.3,-170.95 211.1,-161.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.51,-163.86 216.87,-153.53 208.58,-160.14 214.51,-163.86\"/>\n",
       "</g>\n",
       "<!-- 6436742400 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6436742400</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"160,-146.62 78,-146.62 78,-127.38 160,-127.38 160,-146.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-133.12\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n",
       "</g>\n",
       "<!-- 6436742784&#45;&gt;6436742400 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6436742784&#45;&gt;6436742400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.37,-187.93C167.68,-178.94 150.97,-164.89 138,-153.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.64,-150.78 130.73,-147.03 136.13,-156.14 140.64,-150.78\"/>\n",
       "</g>\n",
       "<!-- 6436744416 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6436744416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"237,-262.75 137,-262.75 137,-243.5 237,-243.5 237,-262.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"187\" y=\"-249.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6436744416&#45;&gt;6436742784 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6436744416&#45;&gt;6436742784</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.33,-243.33C187.58,-236.67 187.93,-227.24 188.25,-218.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.78,-218.98 188.66,-208.86 184.79,-218.72 191.78,-218.98\"/>\n",
       "</g>\n",
       "<!-- 6440611376 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6440611376</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"253,-330 199,-330 199,-298.75 253,-298.75 253,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-304.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 6440611376&#45;&gt;6440611136 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6440611376&#45;&gt;6440611136</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M233.68,-298.32C238.26,-288.48 243.63,-275.19 246,-262.75 248.32,-250.55 250.18,-215.2 245,-188.25 243.35,-179.69 240.38,-170.67 237.26,-162.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.12,-161.44 233.03,-153.56 233.66,-164.14 240.12,-161.44\"/>\n",
       "</g>\n",
       "<!-- 6440611376&#45;&gt;6436744416 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6440611376&#45;&gt;6436744416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.16,-298.42C210.9,-290.44 204.4,-280.55 198.86,-272.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.37,-270.59 192.95,-264.16 195.52,-274.44 201.37,-270.59\"/>\n",
       "</g>\n",
       "<!-- 6440611296 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6440611296</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"146,-30.5 92,-30.5 92,0 146,0 146,-30.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-17\" font-family=\"monospace\" font-size=\"10.00\">z</text>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 6436742064 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6436742064</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"175,-85.75 63,-85.75 63,-66.5 175,-66.5 175,-85.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-72.25\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward4</text>\n",
       "</g>\n",
       "<!-- 6436742064&#45;&gt;6440611296 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>6436742064&#45;&gt;6440611296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119,-66.18C119,-59.65 119,-50.45 119,-41.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.5,-41.78 119,-31.78 115.5,-41.78 122.5,-41.78\"/>\n",
       "</g>\n",
       "<!-- 6436742400&#45;&gt;6436742064 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6436742400&#45;&gt;6436742064</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119,-127.06C119,-119.1 119,-107.19 119,-96.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.5,-97.15 119,-87.15 115.5,-97.15 122.5,-97.15\"/>\n",
       "</g>\n",
       "<!-- 6436746144 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6436746144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"124,-207.5 0,-207.5 0,-188.25 124,-188.25 124,-207.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 6436746144&#45;&gt;6436742400 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6436746144&#45;&gt;6436742400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.66,-187.93C79.11,-179.2 92.19,-165.69 102.61,-154.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"104.82,-157.65 109.26,-148.03 99.79,-152.78 104.82,-157.65\"/>\n",
       "</g>\n",
       "<!-- 6436742304&#45;&gt;6436746144 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6436742304&#45;&gt;6436746144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62,-243.33C62,-236.67 62,-227.24 62,-218.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.5,-218.86 62,-208.86 58.5,-218.86 65.5,-218.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x17fa8fe20>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([0.1,0.2,0.3], requires_grad=True)\n",
    "y = torch.tensor([0.1,0.2,0.3], requires_grad=True).view((3,1))\n",
    "z = x @ y\n",
    "\n",
    "from torchviz import make_dot\n",
    "make_dot((x,y,z), params={'x': x, 'y':y, 'z':z})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28d64c5-df15-4128-9f28-9702eff2c8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: x=[0.24000001], y=[0.0576]\n",
      "Epoch 2: x=[0.192], y=[0.036864]\n",
      "Epoch 3: x=[0.1536], y=[0.02359296]\n",
      "Epoch 4: x=[0.12288], y=[0.0150995]\n",
      "Epoch 5: x=[0.098304], y=[0.00966368]\n",
      "Epoch 6: x=[0.0786432], y=[0.00618475]\n",
      "Epoch 7: x=[0.06291457], y=[0.00395824]\n",
      "Epoch 8: x=[0.05033165], y=[0.00253328]\n",
      "Epoch 9: x=[0.04026532], y=[0.0016213]\n",
      "Epoch 10: x=[0.03221226], y=[0.00103763]\n",
      "Epoch 11: x=[0.02576981], y=[0.00066408]\n",
      "Epoch 12: x=[0.02061584], y=[0.00042501]\n",
      "Epoch 13: x=[0.01649268], y=[0.00027201]\n",
      "Epoch 14: x=[0.01319414], y=[0.00017409]\n",
      "Epoch 15: x=[0.01055531], y=[0.00011141]\n",
      "Epoch 16: x=[0.00844425], y=[7.130537e-05]\n",
      "Epoch 17: x=[0.0067554], y=[4.5635436e-05]\n",
      "Epoch 18: x=[0.00540432], y=[2.920668e-05]\n",
      "Epoch 19: x=[0.00432346], y=[1.8692275e-05]\n",
      "Epoch 20: x=[0.00345877], y=[1.1963056e-05]\n",
      "Epoch 21: x=[0.00276701], y=[7.656356e-06]\n",
      "Epoch 22: x=[0.00221361], y=[4.900068e-06]\n",
      "Epoch 23: x=[0.00177089], y=[3.1360435e-06]\n",
      "Epoch 24: x=[0.00141671], y=[2.007068e-06]\n",
      "Epoch 25: x=[0.00113337], y=[1.2845233e-06]\n",
      "Epoch 26: x=[0.00090669], y=[8.2209493e-07]\n",
      "Epoch 27: x=[0.00072536], y=[5.261407e-07]\n",
      "Epoch 28: x=[0.00058028], y=[3.3673004e-07]\n",
      "Epoch 29: x=[0.00046423], y=[2.1550724e-07]\n",
      "Epoch 30: x=[0.00037138], y=[1.3792463e-07]\n",
      "Epoch 31: x=[0.00029711], y=[8.827176e-08]\n",
      "Epoch 32: x=[0.00023768], y=[5.6493924e-08]\n",
      "Epoch 33: x=[0.00019015], y=[3.6156113e-08]\n",
      "Epoch 34: x=[0.00015212], y=[2.3139911e-08]\n",
      "Epoch 35: x=[0.00012169], y=[1.4809543e-08]\n",
      "Epoch 36: x=[9.735557e-05], y=[9.478107e-09]\n",
      "Epoch 37: x=[7.7884455e-05], y=[6.065988e-09]\n",
      "Epoch 38: x=[6.230756e-05], y=[3.8822323e-09]\n",
      "Epoch 39: x=[4.984605e-05], y=[2.4846287e-09]\n",
      "Epoch 40: x=[3.987684e-05], y=[1.5901622e-09]\n",
      "Epoch 41: x=[3.1901473e-05], y=[1.0177039e-09]\n",
      "Epoch 42: x=[2.5521178e-05], y=[6.5133055e-10]\n",
      "Epoch 43: x=[2.0416943e-05], y=[4.1685155e-10]\n",
      "Epoch 44: x=[1.6333554e-05], y=[2.6678498e-10]\n",
      "Epoch 45: x=[1.3066843e-05], y=[1.7074239e-10]\n",
      "Epoch 46: x=[1.0453475e-05], y=[1.0927513e-10]\n",
      "Epoch 47: x=[8.36278e-06], y=[6.993609e-11]\n",
      "Epoch 48: x=[6.690224e-06], y=[4.4759096e-11]\n",
      "Epoch 49: x=[5.352179e-06], y=[2.864582e-11]\n",
      "Epoch 50: x=[4.2817433e-06], y=[1.8333326e-11]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the function y=x^2 and its derivative\n",
    "x = torch.tensor([0.3], requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "lr = 0.1\n",
    "epochs = 50\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Compute the gradients\n",
    "    y.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    with torch.no_grad():\n",
    "        x -= lr * x.grad\n",
    "\n",
    "        ## Zero out the gradients for the next iteration\n",
    "        x.grad = None\n",
    "\n",
    "    # Recompute the function value with the updated parameters\n",
    "    y = x**2\n",
    "\n",
    "    # Print the current loss\n",
    "    print(f\"Epoch {i+1}: x={x.detach().numpy()}, y={y.detach().numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2dea96-5f76-44f2-927b-3475f41c2b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hr/fqb89fcj36s5zxjlt98289xw0000gn/T/ipykernel_67687/776130399.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(xs)\n",
      "/var/folders/hr/fqb89fcj36s5zxjlt98289xw0000gn/T/ipykernel_67687/776130399.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(ys)\n",
      "/var/folders/hr/fqb89fcj36s5zxjlt98289xw0000gn/T/ipykernel_67687/776130399.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = F.one_hot(torch.tensor(xs), num_classes=27).float()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "W = torch.rand((27,27), requires_grad=True)\n",
    "X = torch.tensor(xs)\n",
    "Y = torch.tensor(ys)\n",
    "\n",
    "X = F.one_hot(torch.tensor(xs), num_classes=27).float()\n",
    "X.requires_grad = True\n",
    "logits = X @ W\n",
    "counts = logits.exp() #exp(z\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(X)), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8824166-20af-4139-b71f-f57c4dd3a7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "073d03f0-3c01-4b67-a15e-e503f0d45dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.4 (20230421.1958)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"311pt\" height=\"625pt\"\n",
       " viewBox=\"0.00 0.00 311.00 624.75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 620.75)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-620.75 307,-620.75 307,4 -4,4\"/>\n",
       "<!-- 5629529120 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5629529120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"85,-616.75 15,-616.75 15,-586.25 85,-586.25 85,-616.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-603.25\" font-family=\"monospace\" font-size=\"10.00\">W</text>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-592\" font-family=\"monospace\" font-size=\"10.00\"> (27, 27)</text>\n",
       "</g>\n",
       "<!-- 5631278480 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5631278480</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-550.25 0,-550.25 0,-531 100,-531 100,-550.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-536.75\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5629529120&#45;&gt;5631278480 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5629529120&#45;&gt;5631278480</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-585.95C50,-578.62 50,-569.62 50,-561.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-561.7 50,-551.7 46.5,-561.7 53.5,-561.7\"/>\n",
       "</g>\n",
       "<!-- 5509584256 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5509584256</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"215,-616.75 121,-616.75 121,-586.25 215,-586.25 215,-616.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-603.25\" font-family=\"monospace\" font-size=\"10.00\">X</text>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-592\" font-family=\"monospace\" font-size=\"10.00\"> (228146, 27)</text>\n",
       "</g>\n",
       "<!-- 5631278384 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5631278384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"218,-550.25 118,-550.25 118,-531 218,-531 218,-550.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-536.75\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 5509584256&#45;&gt;5631278384 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5509584256&#45;&gt;5631278384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168,-585.95C168,-578.62 168,-569.62 168,-561.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.5,-561.7 168,-551.7 164.5,-561.7 171.5,-561.7\"/>\n",
       "</g>\n",
       "<!-- 5629528560 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>5629528560</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"303,-616.75 233,-616.75 233,-586.25 303,-586.25 303,-616.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-603.25\" font-family=\"monospace\" font-size=\"10.00\">Y</text>\n",
       "<text text-anchor=\"middle\" x=\"268\" y=\"-592\" font-family=\"monospace\" font-size=\"10.00\"> (228146)</text>\n",
       "</g>\n",
       "<!-- 5509582336 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5509582336</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"102,-439.75 8,-439.75 8,-409.25 102,-409.25 102,-439.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-426.25\" font-family=\"monospace\" font-size=\"10.00\">logits</text>\n",
       "<text text-anchor=\"middle\" x=\"55\" y=\"-415\" font-family=\"monospace\" font-size=\"10.00\"> (228146, 27)</text>\n",
       "</g>\n",
       "<!-- 5631278720 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5631278720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"150,-495 68,-495 68,-475.75 150,-475.75 150,-495\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-481.5\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278720&#45;&gt;5509582336 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5631278720&#45;&gt;5509582336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.8,-475.43C94.14,-468.17 84.44,-457.6 75.68,-448.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.67,-446.04 69.33,-441.03 73.51,-450.77 78.67,-446.04\"/>\n",
       "</g>\n",
       "<!-- 5631278432 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>5631278432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-434.12 120,-434.12 120,-414.88 208,-414.88 208,-434.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"164\" y=\"-420.62\" font-family=\"monospace\" font-size=\"10.00\">ExpBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278720&#45;&gt;5631278432 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5631278720&#45;&gt;5631278432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.35,-475.43C125.51,-466.7 138.13,-453.19 148.19,-442.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.3,-445.22 154.57,-435.53 145.18,-440.44 150.3,-445.22\"/>\n",
       "</g>\n",
       "<!-- 5631278384&#45;&gt;5631278720 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5631278384&#45;&gt;5631278720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.99,-530.59C149.46,-522.9 137,-511.65 126.78,-502.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.63,-500.38 119.86,-496.27 124.94,-505.57 129.63,-500.38\"/>\n",
       "</g>\n",
       "<!-- 5631278480&#45;&gt;5631278720 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5631278480&#45;&gt;5631278720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.01,-530.59C68.54,-522.9 81,-511.65 91.22,-502.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"93.06,-505.57 98.14,-496.27 88.37,-500.38 93.06,-505.57\"/>\n",
       "</g>\n",
       "<!-- 5629508672 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>5629508672</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"136,-373.25 42,-373.25 42,-342.75 136,-342.75 136,-373.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-359.75\" font-family=\"monospace\" font-size=\"10.00\">counts</text>\n",
       "<text text-anchor=\"middle\" x=\"89\" y=\"-348.5\" font-family=\"monospace\" font-size=\"10.00\"> (228146, 27)</text>\n",
       "</g>\n",
       "<!-- 5631278432&#45;&gt;5629508672 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5631278432&#45;&gt;5629508672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M153.57,-414.53C143.37,-405.76 127.45,-392.07 113.97,-380.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116.83,-377.46 106.97,-373.59 112.27,-382.76 116.83,-377.46\"/>\n",
       "</g>\n",
       "<!-- 5631377568 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>5631377568</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-306.75 156,-306.75 156,-287.5 244,-287.5 244,-306.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-293.25\" font-family=\"monospace\" font-size=\"10.00\">DivBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278432&#45;&gt;5631377568 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>5631278432&#45;&gt;5631377568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.72,-414.43C168.68,-399.38 175.15,-368.36 183,-342.75 185.61,-334.24 189.02,-325.01 192.1,-317.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.62,-318.81 196.12,-308.23 189.13,-316.19 195.62,-318.81\"/>\n",
       "</g>\n",
       "<!-- 5631377664 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>5631377664</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"280,-367.62 192,-367.62 192,-348.38 280,-348.38 280,-367.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"236\" y=\"-354.12\" font-family=\"monospace\" font-size=\"10.00\">SumBackward1</text>\n",
       "</g>\n",
       "<!-- 5631278432&#45;&gt;5631377664 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>5631278432&#45;&gt;5631377664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174.01,-414.53C185.4,-404.33 204.23,-387.46 218.15,-374.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.96,-378.17 225.07,-368.89 215.29,-372.96 219.96,-378.17\"/>\n",
       "</g>\n",
       "<!-- 5629540080 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>5629540080</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"227,-30.5 173,-30.5 173,0 227,0 227,-30.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-17\" font-family=\"monospace\" font-size=\"10.00\">loss</text>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 5631278672 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>5631278672</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-85.75 156,-85.75 156,-66.5 244,-66.5 244,-85.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-72.25\" font-family=\"monospace\" font-size=\"10.00\">NegBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278672&#45;&gt;5629540080 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>5631278672&#45;&gt;5629540080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-66.18C200,-59.65 200,-50.45 200,-41.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-41.78 200,-31.78 196.5,-41.78 203.5,-41.78\"/>\n",
       "</g>\n",
       "<!-- 5631278864 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>5631278864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"247,-141 153,-141 153,-121.75 247,-121.75 247,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-127.5\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278864&#45;&gt;5631278672 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>5631278864&#45;&gt;5631278672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-121.58C200,-114.92 200,-105.49 200,-97.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-97.11 200,-87.11 196.5,-97.11 203.5,-97.11\"/>\n",
       "</g>\n",
       "<!-- 5631278144 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>5631278144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"244,-196.25 156,-196.25 156,-177 244,-177 244,-196.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-182.75\" font-family=\"monospace\" font-size=\"10.00\">LogBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278144&#45;&gt;5631278864 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>5631278144&#45;&gt;5631278864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-176.83C200,-170.17 200,-160.74 200,-152.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-152.36 200,-142.36 196.5,-152.36 203.5,-152.36\"/>\n",
       "</g>\n",
       "<!-- 5631278096 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>5631278096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"250,-251.5 150,-251.5 150,-232.25 250,-232.25 250,-251.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-238\" font-family=\"monospace\" font-size=\"10.00\">IndexBackward0</text>\n",
       "</g>\n",
       "<!-- 5631278096&#45;&gt;5631278144 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>5631278096&#45;&gt;5631278144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-232.08C200,-225.42 200,-215.99 200,-207.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-207.61 200,-197.61 196.5,-207.61 203.5,-207.61\"/>\n",
       "</g>\n",
       "<!-- 5631377568&#45;&gt;5631278096 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>5631377568&#45;&gt;5631278096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200,-287.33C200,-280.67 200,-271.24 200,-262.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.5,-262.86 200,-252.86 196.5,-262.86 203.5,-262.86\"/>\n",
       "</g>\n",
       "<!-- 5631377664&#45;&gt;5631377568 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>5631377664&#45;&gt;5631377568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.53,-348.06C225.4,-339.67 217.58,-326.88 211.14,-316.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.72,-314.86 205.52,-308.15 207.75,-318.51 213.72,-314.86\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x14fa67eb0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz \n",
    "# Compute gradients\n",
    "\n",
    "# Visualize the gradients\n",
    "torchviz.make_dot((W, X, Y, logits, counts, loss), params=dict(W=W, X=X, Y=Y, logits=logits, counts=counts, loss=loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805ba277-0a63-4413-bd2b-bd6acfd24694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utilities import read_file #helper function for reading in files.\n",
    "\n",
    "names = list(read_file('names.txt'))\n",
    "C = sorted(list(set(''.join(names))))\n",
    "ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "ctoi['.'] = 0\n",
    "itos = {i:s for s,i in ctoi.items()}\n",
    "\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for n in names:\n",
    "    n = ['.'] + list(n) +['.']\n",
    "    for c1, c2 in zip(n, n[1:]): \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "W = torch.rand((27,27), requires_grad=True)\n",
    "X = torch.tensor(xs)\n",
    "Y = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a3fad7-7390-4634-a0b3-01884b0b4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "X = F.one_hot(torch.tensor(xs), num_classes=27).float()\n",
    "X.requires_grad = True\n",
    "logits = X @ W\n",
    "counts = logits.exp()w\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(X)), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3606e90f-ef5f-406b-af59-304504c1069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3440961837768555\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478cb9f3-3b78-449b-8e66-6d6b81d1afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "636d4d8c-9012-4666-a9ef-1d85eb8bf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d3076-4b0e-4178-b37c-470266b0effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4b7192-ccfd-4b80-8b72-3e255af1ab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4689276218414307\n",
      "2.468898296356201\n",
      "2.4688689708709717\n",
      "2.468839645385742\n",
      "2.4688103199005127\n",
      "2.4687814712524414\n",
      "2.468752384185791\n",
      "2.4687235355377197\n",
      "2.4686949253082275\n",
      "2.4686665534973145\n",
      "2.468637704849243\n",
      "2.468609571456909\n",
      "2.468581199645996\n",
      "2.468553066253662\n",
      "2.468524932861328\n",
      "2.468496799468994\n",
      "2.4684689044952393\n",
      "2.4684410095214844\n",
      "2.4684135913848877\n",
      "2.468385934829712\n",
      "2.4683585166931152\n",
      "2.4683313369750977\n",
      "2.468303918838501\n",
      "2.4682769775390625\n",
      "2.468249559402466\n",
      "2.4682226181030273\n",
      "2.468195915222168\n",
      "2.4681689739227295\n",
      "2.46814227104187\n",
      "2.46811580657959\n",
      "2.4680893421173096\n",
      "2.46806263923645\n",
      "2.468036651611328\n",
      "2.468010187149048\n",
      "2.467984199523926\n",
      "2.4679579734802246\n",
      "2.4679322242736816\n",
      "2.4679064750671387\n",
      "2.4678807258605957\n",
      "2.4678549766540527\n",
      "2.467829465866089\n",
      "2.467803716659546\n",
      "2.467778205871582\n",
      "2.4677534103393555\n",
      "2.4677278995513916\n",
      "2.467702627182007\n",
      "2.4676778316497803\n",
      "2.4676527976989746\n",
      "2.467628240585327\n",
      "2.4676029682159424\n",
      "2.467578411102295\n",
      "2.4675538539886475\n",
      "2.467529535293579\n",
      "2.4675049781799316\n",
      "2.467480421066284\n",
      "2.467456102371216\n",
      "2.4674320220947266\n",
      "2.4674079418182373\n",
      "2.467384099960327\n",
      "2.467360258102417\n",
      "2.4673361778259277\n",
      "2.4673123359680176\n",
      "2.4672887325286865\n",
      "2.4672651290893555\n",
      "2.4672412872314453\n",
      "2.4672181606292725\n",
      "2.4671947956085205\n",
      "2.4671714305877686\n",
      "2.4671483039855957\n",
      "2.467125177383423\n",
      "2.46710205078125\n",
      "2.4670791625976562\n",
      "2.4670562744140625\n",
      "2.4670333862304688\n",
      "2.467010498046875\n",
      "2.4669876098632812\n",
      "2.4669651985168457\n",
      "2.466942548751831\n",
      "2.4669201374053955\n",
      "2.46689772605896\n",
      "2.4668753147125244\n",
      "2.466852903366089\n",
      "2.4668309688568115\n",
      "2.466809034347534\n",
      "2.4667868614196777\n",
      "2.4667651653289795\n",
      "2.466742992401123\n",
      "2.466721534729004\n",
      "2.4666998386383057\n",
      "2.4666781425476074\n",
      "2.46665620803833\n",
      "2.466634750366211\n",
      "2.466613531112671\n",
      "2.4665918350219727\n",
      "2.4665703773498535\n",
      "2.4665493965148926\n",
      "2.4665279388427734\n",
      "2.4665071964263916\n",
      "2.4664862155914307\n",
      "2.4664649963378906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 10\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    X = F.one_hot(torch.tensor(xs), num_classes=27).float()\n",
    "    X.requires_grad = True\n",
    "    logits = X @ W\n",
    "    counts = logits.exp() #exp(z\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(X)), Y].log().mean()\n",
    "    \n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    with torch.no_grad():\n",
    "        W += -lr * W.grad\n",
    "        W.grad = None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f4219e-8c31-40d0-84a8-93a061bd55ef",
   "metadata": {},
   "source": [
    "\n",
    "# NOTES:\n",
    "\n",
    "- Leave Backprop off the table until a later date, existing network isn't the best example of a classic NN architecture. Only one non-linear activation in output layer. Showing how to perform backprop over a non-standard NN might be a bit confusing.\n",
    "\n",
    "- Softmax is going to make the math a bit more complex since most pytorch implementations don't explicitly use the softmax but rather the pre-softmax values that are computed at the same time with the cross entropy loss. This distinction is important in pytorch and is something folks are likely to run into when building classifiers.\n",
    "\n",
    "- Vector notation for partial derivatives can be confusing unless represented explicitly using bmatrix. Also not sure if I should include the idea of the jacobian since it is not explicitly used in the autograd package for computing gradients. At the same time introspection of the jacobian does allow better insights into how changes in the inputs of the models affect the outputs and similarly how the inputs impact the parameters of the model.\n",
    "\n",
    "### Backpropagation, Chain Rule and the Jacobian [DRAFT]\n",
    "\n",
    "We want to find the optimal parameters $\\mathbf{W}$ for our neural network that minimize the loss function, which measures how well the network is performing. However, the loss function $\\mathcal{L}$ is not directly defined in terms of the weight matrix $\\mathbf{W}$, so we cannot compute the gradient of the loss function with respect to the weights directly.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{T - 1} \\sum_{(c_j, c_i) \\in \\mathcal{D}} \\log \\hat{y}_i,\n",
    "$$\n",
    "\n",
    "\n",
    "The challenge is to find a way to express the gradient of the loss function with respect to $\\mathbf{W}$, so that we can update the weights using gradient-based optimization methods, such as gradient descent.\n",
    "\n",
    "Let's provide a more mathematical illustration of the chain rule by breaking down the gradient of the loss function with respect to the weights, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}$, using intermediate variables.\n",
    "\n",
    "\n",
    "We will use the same neural network architecture as above.\n",
    "$$\n",
    "\\mathbf{x} = \\phi(c_j),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \\mathbf{W} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\text{softmax}(\\mathbf{h}),\n",
    "$$\n",
    "\n",
    "To compute the gradient of the loss function with respect to the weights, we need to apply the chain rule through these layers. For example, let's compute the gradient of the loss function with respect to the weight matrix $\\mathbf{W}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\sum_{i=1}^{V} \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "Here, the individual gradients are:\n",
    "\n",
    "1. Gradient of the loss function with respect to the predicted probabilities, $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i} = -\\frac{1}{T - 1} \\frac{1}{\\hat{y}_i}\n",
    "$$\n",
    "\n",
    "2. Gradient of the predicted probabilities with respect to the pre-softmax values, $\\frac{\\partial \\hat{y}_i}{\\partial h_k}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_i}{\\partial h_k} = \\begin{cases}\n",
    "\\hat{y}_i(1-\\hat{y}_k) & \\text{if } i=k \\\\\n",
    "-\\hat{y}_i \\hat{y}_k & \\text{if } i \\ne k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. Gradient of the pre-softmax values with respect to the weight matrix, $\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{W}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_k}{\\partial \\mathbf{W}} = \\mathbf{a}\n",
    "$$\n",
    "\n",
    "By multiplying these individual gradients together and summing over all possible characters, we can compute the gradient of the loss function with respect to $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f82f2-76a5-4be7-b2ac-f6de30d9b202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
