{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "155b2aee-217d-4724-8e5d-baefb2ae3175",
   "metadata": {},
   "source": [
    "# Pytorch Modules and Builtin Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f1e3e-a816-4635-8c61-8fbb5390bf1e",
   "metadata": {},
   "source": [
    "## Torch's `nn.Parameter`\n",
    "\n",
    "In PyTorch, any tensor that needs to be trained can be wrapped with the `nn.Parameter` class, which makes it a trainable parameter. nn.Parameter is a subclass of the `torch.Tensor` class, which means that it has all the properties and methods of a tensor, but with additional functionality for optimization.\n",
    "\n",
    "When a tensor is wrapped with nn.Parameter, it is automatically added to the list of parameters that can be optimized by an optimizer such as stochastic gradient descent (SGD). This means that when you call `backward()` on your loss tensor, the gradients will be computed for all the parameters in your model, including those wrapped with nn.Parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2944b725-83e9-4f26-a631-40aa8cdbf18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3489, 0.4017, 0.0223, 0.1689, 0.2939, 0.5185, 0.6977, 0.8000, 0.1610,\n",
      "         0.2823],\n",
      "        [0.6816, 0.9152, 0.3971, 0.8742, 0.4194, 0.5529, 0.9527, 0.0362, 0.1852,\n",
      "         0.3734],\n",
      "        [0.3051, 0.9320, 0.1759, 0.2698, 0.1507, 0.0317, 0.2081, 0.9298, 0.7231,\n",
      "         0.7423],\n",
      "        [0.5263, 0.2437, 0.5846, 0.0332, 0.1387, 0.2422, 0.8155, 0.7932, 0.2783,\n",
      "         0.4820],\n",
      "        [0.8198, 0.9971, 0.6984, 0.5675, 0.8352, 0.2056, 0.5932, 0.1123, 0.1535,\n",
      "         0.2417]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.3489, 0.4017, 0.0223, 0.1689, 0.2939, 0.5185, 0.6977, 0.8000, 0.1610,\n",
      "         0.2823],\n",
      "        [0.6816, 0.9152, 0.3971, 0.8742, 0.4194, 0.5529, 0.9527, 0.0362, 0.1852,\n",
      "         0.3734],\n",
      "        [0.3051, 0.9320, 0.1759, 0.2698, 0.1507, 0.0317, 0.2081, 0.9298, 0.7231,\n",
      "         0.7423],\n",
      "        [0.5263, 0.2437, 0.5846, 0.0332, 0.1387, 0.2422, 0.8155, 0.7932, 0.2783,\n",
      "         0.4820],\n",
      "        [0.8198, 0.9971, 0.6984, 0.5675, 0.8352, 0.2056, 0.5932, 0.1123, 0.1535,\n",
      "         0.2417]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "W = torch.rand(10, 10)\n",
    "W.requires_grad = True\n",
    "X = F.one_hot(torch.tensor([1,2,3,4,5]), num_classes=10).float()\n",
    "\n",
    "print(X @ W)\n",
    "\n",
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "\n",
    "W = nn.Parameter(torch.rand(10, 10))\n",
    "X = F.one_hot(torch.tensor([1,2,3,4,5]), num_classes=10).float()\n",
    "\n",
    "print(X @ W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709a352-67a4-4426-b07e-3641a728ab9e",
   "metadata": {},
   "source": [
    "## Torch's `nn.Module`\n",
    "\n",
    "The `nn.Module` class is a fundamental building block for creating complex deep learning models. It provides a convenient way to organize and encapsulate all the trainable parameters and operations of a deep learning model.\n",
    "\n",
    "`nn.Module` is designed to make it easy to build complex neural networks by allowing you to define layers, activations, loss functions, and other components as modules. It provides a set of pre-defined methods for forward propagation, backward propagation, and optimization that can be easily customized to fit your specific needs.\n",
    "\n",
    "Here is an example of how to create an nn.Module:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5042b5d3-17c0-4a94-9c23-17ab99a2ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, A, B_rows, C_cols):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.A = A\n",
    "        self.B = nn.Parameter(torch.randn(B_rows, A.size(1)))\n",
    "        self.C = nn.Parameter(torch.randn(A.size(0), C_cols))\n",
    "\n",
    "    def forward(self):\n",
    "        return torch.matmul(self.C, self.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f929494-0cf8-4727-860f-29964d899a3a",
   "metadata": {},
   "source": [
    "### Torch's `torch.optim`\n",
    "\n",
    "PyTorch optimizer is a class in the PyTorch library that helps in optimizing the parameters of a neural network during the training process. It provides several optimization algorithms, such as Stochastic Gradient Descent (SGD), Adam, Adagrad, RMSProp, etc., to update the weights and biases of the neural network to minimize the loss function.\n",
    "\n",
    "\n",
    "The `optim.step()` function is called after computing the gradients of the loss function with respect to the model parameters using `loss.backward()`. It updates the model parameters similarly to:\n",
    "\n",
    "-------------------------\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    W += -lr * W.grad\n",
    "    W.grad = None\n",
    "```\n",
    "-------------------------\n",
    "\n",
    "When defining an optimizer we provide it with all the parameters over which we would like to perform gradient descent `optim.SGD(model.parameters(), lr=1)` as well as a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "79b0e631-368f-4862-96f5-596ade66ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 1.0932\n",
      "Epoch [200/10000], Loss: 0.4378\n",
      "Epoch [300/10000], Loss: 0.3379\n",
      "Epoch [400/10000], Loss: 0.3049\n",
      "Epoch [500/10000], Loss: 0.2875\n",
      "Epoch [600/10000], Loss: 0.2751\n",
      "Epoch [700/10000], Loss: 0.2648\n",
      "Epoch [800/10000], Loss: 0.2556\n",
      "Epoch [900/10000], Loss: 0.2470\n",
      "Epoch [1000/10000], Loss: 0.2390\n",
      "Epoch [1100/10000], Loss: 0.2314\n",
      "Epoch [1200/10000], Loss: 0.2242\n",
      "Epoch [1300/10000], Loss: 0.2175\n",
      "Epoch [1400/10000], Loss: 0.2111\n",
      "Epoch [1500/10000], Loss: 0.2051\n",
      "Epoch [1600/10000], Loss: 0.1994\n",
      "Epoch [1700/10000], Loss: 0.1940\n",
      "Epoch [1800/10000], Loss: 0.1889\n",
      "Epoch [1900/10000], Loss: 0.1840\n",
      "Epoch [2000/10000], Loss: 0.1794\n",
      "Epoch [2100/10000], Loss: 0.1751\n",
      "Epoch [2200/10000], Loss: 0.1709\n",
      "Epoch [2300/10000], Loss: 0.1669\n",
      "Epoch [2400/10000], Loss: 0.1632\n",
      "Epoch [2500/10000], Loss: 0.1596\n",
      "Epoch [2600/10000], Loss: 0.1562\n",
      "Epoch [2700/10000], Loss: 0.1529\n",
      "Epoch [2800/10000], Loss: 0.1498\n",
      "Epoch [2900/10000], Loss: 0.1469\n",
      "Epoch [3000/10000], Loss: 0.1440\n",
      "Epoch [3100/10000], Loss: 0.1413\n",
      "Epoch [3200/10000], Loss: 0.1387\n",
      "Epoch [3300/10000], Loss: 0.1362\n",
      "Epoch [3400/10000], Loss: 0.1338\n",
      "Epoch [3500/10000], Loss: 0.1316\n",
      "Epoch [3600/10000], Loss: 0.1294\n",
      "Epoch [3700/10000], Loss: 0.1273\n",
      "Epoch [3800/10000], Loss: 0.1253\n",
      "Epoch [3900/10000], Loss: 0.1233\n",
      "Epoch [4000/10000], Loss: 0.1214\n",
      "Epoch [4100/10000], Loss: 0.1196\n",
      "Epoch [4200/10000], Loss: 0.1179\n",
      "Epoch [4300/10000], Loss: 0.1163\n",
      "Epoch [4400/10000], Loss: 0.1146\n",
      "Epoch [4500/10000], Loss: 0.1131\n",
      "Epoch [4600/10000], Loss: 0.1116\n",
      "Epoch [4700/10000], Loss: 0.1102\n",
      "Epoch [4800/10000], Loss: 0.1088\n",
      "Epoch [4900/10000], Loss: 0.1074\n",
      "Epoch [5000/10000], Loss: 0.1061\n",
      "Epoch [5100/10000], Loss: 0.1048\n",
      "Epoch [5200/10000], Loss: 0.1036\n",
      "Epoch [5300/10000], Loss: 0.1024\n",
      "Epoch [5400/10000], Loss: 0.1013\n",
      "Epoch [5500/10000], Loss: 0.1001\n",
      "Epoch [5600/10000], Loss: 0.0990\n",
      "Epoch [5700/10000], Loss: 0.0980\n",
      "Epoch [5800/10000], Loss: 0.0970\n",
      "Epoch [5900/10000], Loss: 0.0960\n",
      "Epoch [6000/10000], Loss: 0.0950\n",
      "Epoch [6100/10000], Loss: 0.0941\n",
      "Epoch [6200/10000], Loss: 0.0931\n",
      "Epoch [6300/10000], Loss: 0.0922\n",
      "Epoch [6400/10000], Loss: 0.0914\n",
      "Epoch [6500/10000], Loss: 0.0905\n",
      "Epoch [6600/10000], Loss: 0.0897\n",
      "Epoch [6700/10000], Loss: 0.0889\n",
      "Epoch [6800/10000], Loss: 0.0881\n",
      "Epoch [6900/10000], Loss: 0.0874\n",
      "Epoch [7000/10000], Loss: 0.0866\n",
      "Epoch [7100/10000], Loss: 0.0859\n",
      "Epoch [7200/10000], Loss: 0.0852\n",
      "Epoch [7300/10000], Loss: 0.0845\n",
      "Epoch [7400/10000], Loss: 0.0838\n",
      "Epoch [7500/10000], Loss: 0.0831\n",
      "Epoch [7600/10000], Loss: 0.0825\n",
      "Epoch [7700/10000], Loss: 0.0819\n",
      "Epoch [7800/10000], Loss: 0.0812\n",
      "Epoch [7900/10000], Loss: 0.0806\n",
      "Epoch [8000/10000], Loss: 0.0800\n",
      "Epoch [8100/10000], Loss: 0.0795\n",
      "Epoch [8200/10000], Loss: 0.0789\n",
      "Epoch [8300/10000], Loss: 0.0783\n",
      "Epoch [8400/10000], Loss: 0.0778\n",
      "Epoch [8500/10000], Loss: 0.0773\n",
      "Epoch [8600/10000], Loss: 0.0767\n",
      "Epoch [8700/10000], Loss: 0.0762\n",
      "Epoch [8800/10000], Loss: 0.0757\n",
      "Epoch [8900/10000], Loss: 0.0752\n",
      "Epoch [9000/10000], Loss: 0.0747\n",
      "Epoch [9100/10000], Loss: 0.0743\n",
      "Epoch [9200/10000], Loss: 0.0738\n",
      "Epoch [9300/10000], Loss: 0.0734\n",
      "Epoch [9400/10000], Loss: 0.0729\n",
      "Epoch [9500/10000], Loss: 0.0725\n",
      "Epoch [9600/10000], Loss: 0.0720\n",
      "Epoch [9700/10000], Loss: 0.0716\n",
      "Epoch [9800/10000], Loss: 0.0712\n",
      "Epoch [9900/10000], Loss: 0.0708\n",
      "Epoch [10000/10000], Loss: 0.0704\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "A = torch.randint(0,3,(100,100)).float()\n",
    "B_rows = 60\n",
    "C_cols = 60\n",
    "\n",
    "model = MatrixFactorization(A, B_rows, C_cols)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    A_approx = model()\n",
    "    loss = criterion(A_approx, A)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cd74d-f571-4836-8c01-e7fdeeb98764",
   "metadata": {},
   "source": [
    "## Torch's `nn.Embedding`\n",
    "\n",
    "`nn.Embedding` is a PyTorch module that maps discrete tokens (e.g., words, characters, or subwords) to vectors of fixed size in a continuous space. These embeddings can be considered as a lookup table that converts an index (corresponding to a specific word) into a dense vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0cc72045-b959-40c7-a486-4ad9a623ef49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,\n",
       "          0.6408,  0.4412],\n",
       "        [-0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867,\n",
       "         -0.6731,  0.8728]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# Initialize the weight matrix using the normal distribution\n",
    "W = torch.randn((10,10))\n",
    "x = torch.tensor([3, 4])\n",
    "\n",
    "X = F.one_hot(x, num_classes=10).float()\n",
    "X @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "83ff448e-c046-44de-b506-f5e21c1ca2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9463, -0.8437, -0.6136,  0.0316, -0.4927,  0.2484,  0.4397,  0.1124,\n",
       "          0.6408,  0.4412],\n",
       "        [-0.1023,  0.7924, -0.2897,  0.0525,  0.5229,  2.3022, -1.4689, -1.5867,\n",
       "         -0.6731,  0.8728]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "E = nn.Embedding(10,10)\n",
    "x = torch.tensor([3, 4])\n",
    "E(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88479bf8-153e-4523-8b6b-81af46967b5e",
   "metadata": {},
   "source": [
    "# Torch's `nn.Linear`\n",
    "\n",
    "`nn.Linear` performs an affine transformation on the input data. Given an input tensor `x`, it computes the output `y` as follows:\n",
    "\n",
    "$$\n",
    "y = \\mathbf{W}x + b\n",
    "$$\n",
    "\n",
    "Here, `W` represents the weight matrix, `x` is the input tensor, `b` is the bias vector, and `y` is the output tensor. Both the weight matrix and the bias vector are learnable parameters of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9e4027f2-7632-49d6-96d5-4d6b3b1db761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2327, -0.2094,  0.2366, -0.1456, -0.2953,  0.0427,  0.0955,  0.2620,\n",
       "         -0.3154,  0.1182],\n",
       "        [-0.1218, -0.1304, -0.0510, -0.2209, -0.2285,  0.2120,  0.1736,  0.2752,\n",
       "          0.0592, -0.3130]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Initialize the weight matrix using the normal distribution\n",
    "W = torch.randn(10, 10)\n",
    "x = torch.tensor([3, 4])\n",
    "\n",
    "X = F.one_hot(x, num_classes=10).float()\n",
    "X @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "52d52797-6b89-453c-8190-835fd7b5d4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2327, -0.2094,  0.2366, -0.1456, -0.2953,  0.0427,  0.0955,  0.2620,\n",
       "         -0.3154,  0.1182],\n",
       "        [-0.1218, -0.1304, -0.0510, -0.2209, -0.2285,  0.2120,  0.1736,  0.2752,\n",
       "          0.0592, -0.3130]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "linear_layer = nn.Linear(in_features=10, out_features=10, bias=False)\n",
    "linear_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d6139-54ce-40d3-a469-a0861f56d300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
