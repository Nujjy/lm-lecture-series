{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c2d0ff-1ca0-449a-bfac-6789f8fc0e08",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4738c-9816-4dd2-b859-0bd56ad08840",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47bf548-ddd9-4c31-97c2-48492b905fbe",
   "metadata": {},
   "source": [
    "# Language Modelling in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6ef31-53bd-4745-a63a-8e9b42e942b1",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "# Overview\n",
    "\n",
    "Language modeling is a fundamental task in natural language processing (NLP) that involves predicting or estimating the likelihood of a sequence of words or tokens in a language. The main goal of language modeling is to capture the structure and patterns of language in order to generate coherent and natural-sounding text.\n",
    "\n",
    "Let's take the following sentence:\n",
    "***<p style=\"text-align: center\">Anuj is an ____***</p>\n",
    "\n",
    "\n",
    "What do you think the next word or few words would be in this sentence?\n",
    "\n",
    "| Next Phrase    | Likely?          |\n",
    "| -------------- | ---------------- |\n",
    "| idiot          | high likely      |\n",
    "| moron          | likely           |\n",
    "| a muppet       | somewhat likely  |\n",
    "| February       | unlikely         |\n",
    "| .              | .                |\n",
    "| .              | .                |\n",
    "| asdasdasd      | highly unlikely  |\n",
    "| awesome person | practically zero |\n",
    "\n",
    "Inuitively from our own understanding of natural language we can form some opinion of what words or characters are most likely to come next. Some of this is based on the fundemental constructs of natural language like syntax and grammer, but I believe our intuition is largely formed inductively through the confluence of choice words.\n",
    "\n",
    "## Definition\n",
    "\n",
    "\n",
    "Consider a sequence of random variables $X$ where $\\langle X_1, X_i, X_j, \\ldots, X_n\\rangle \\quad \\forall i,j \\in {1, 2, \\ldots, n},\\ i < j \\Rightarrow X_i \\neq X_j$ Each random variable $X_i$ can take any value in a finite set ${\\mathcal{V}}$. \n",
    "\n",
    "We can model the probability of any sequence of words $w_1 \\ldots w_n$, where $n \\geq 1$ and $w_i \\in V$ for $i = 1 \\ldots n$ as the following joint probability distribution.\n",
    "\n",
    "\\begin{align}\n",
    "P(X_1 = w_1, X_2 = w_2, ..., X_n = w_n)\n",
    "\\end{align}\n",
    "\n",
    "<div class=\"definition\">\n",
    "<b>Definition 1:</b> A language model consists of a finite set ${\\mathcal{V}}$, and a function $P(X_1 = w_1, X_2 = w_2, ..., X_n = w_n)$ such that:\n",
    "\n",
    "1. For any $\\langle w_1, w_2, ..., w_n \\rangle \\in {\\mathcal{V}}$ , $P(X_1 = w_1, X_2 = w_2, ..., X_n = w_n) \\geq 0$\n",
    "2. In addition,\n",
    "\\begin{align}\n",
    "\\sum_{w_1...w_i \\in {\\mathcal{V}}} P(X_1 = w_1, X_2 = w_2, ..., X_n = w_n) = 1\n",
    "\\end{align}\n",
    "</div>\n",
    "\n",
    "\n",
    "Formally given a sequence of tokens or words $w_1, w_2, ..., w_n$, a language model calculates the probability of observing that sequence. $P(X_1 = w_1, X_2 = w_2, ..., X_n = w_n)$. More succintly we will express this henceforth as $P(w_1, w_2, ..., w_n)$\n",
    "\n",
    "\n",
    "However estimating the joint probability directly would require ${\\mathcal{V}}^n$ possible sequences of the form $w_1... w_n$ and therefore not feasible. Luckily this probability can be decomposed using the chain rule of probability to link computing the joint probability of a sequence and computing the conditional probability of a word given previous words. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(w_1, w_2, ..., w_n) &=P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdot ... \\cdot P(w_n | w_1, w_2, ..., w_{n-1}) \\\\\n",
    " &= \\prod_{i=1}^{n} P(w_i | w_{1},, ..., w_{i-1}) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $P(w_i|w_1, ..., w_{i-1})$ is the probability of observing the $i$-th word given the previous words.\n",
    "\n",
    "By breaking down the probability of the sentence in this way, we only need to estimate the probability of each word given the previous words rather than estimating the probabilities of all possible word sequences. This decomposition can also help to address the issue of data sparsity, as we only need to estimate the probabilities of each word in the context of its preceding words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a290432-03cf-4a21-84c7-1f176a09094d",
   "metadata": {},
   "source": [
    "## N-Grams and the Markov Assumption\n",
    "\n",
    "One way to approach language modeling is through the use of n-grams, which are sequences of n words that occur together in a text. \n",
    "\n",
    "### N-Gram Language Model:\n",
    "$$P(w_1,w_2,...,w_n) = \\prod_{i=1}^n P(w_i | w_{1}, ..., w_{i-N})$$\n",
    "\n",
    "N-grams instrinsicly make use of the markov assumption. The Markov assumption is a key assumption in many statistical and probabilistic models that states that the probability of a future event only depends on the current state or condition, and not on the entire history of past events. In the context of language modeling:\n",
    "    \n",
    "<div class=\"definition\">\n",
    "<b>Definition 2:</b> The Markov Assumption states for observing a finite sequence $\\{x_1,...,x_n\\} \\in {\\mathcal{X}}$ the probability of observing the event $P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n).\\\\\\\\$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) &=P(X_1 = x_1) \\cdot P(X_2 = x_2 | X_1 = x_1) \\cdot ... \\cdot P(X_n = x_n | X_1 = x_1, X_2 = x_2, ..., X_1 = x_{n-1}) \\\\\n",
    "&= \\prod_{i=1}^{n} P(X_i = x_i | X_{1} = x_{1}, ..., X_{i-1} = x_{i-1}) \\\\\n",
    "&\\approx \\prod_{i=1}^{n} P(X_i = x_i | X_{i-1} = x_{i-1})\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Using this property we can create more compact models such as the unigram, bigram and trigram language models which are commonly used in a variety of application in NLP.\n",
    "    \n",
    "### Unigram Language Model:\n",
    "$$P(w_1,w_2,...,w_n) \\approx \\prod_{i=1}^n P(w_i)$$\n",
    "In the unigram model, the probability of a word in a sequence is independent of the context or any other words in the sequence. The probability of a sequence is simply the product of the probabilities of each individual word.\n",
    "\n",
    "### Bigram Language Model:\n",
    "$$P(w_1,w_2,...,w_n) \\approx \\prod_{i=1}^n P(w_i|w_{i-1})$$\n",
    "In the bigram model, the probability of a word in a sequence depends only on the previous word in the sequence. The probability of a sequence is the product of the probabilities of each word given its preceding word.\n",
    "\n",
    "### Trigram Language Model:\n",
    "$$P(w_1,w_2,...,w_n) \\approx \\prod_{i=1}^n P(w_i|w_{i-1},w_{i-2})$$\n",
    "In the trigram model, the probability of a word in a sequence depends on the previous two words in the sequence. The probability of a sequence is the product of the probabilities of each word given its preceding two words.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbb459-3543-4852-80d5-2e9852d19049",
   "metadata": {},
   "source": [
    "## Maximum Liklihood Estimation\n",
    "\n",
    "The maximum likelihood estimate (MLE) is a commonly used method to estimate the parameters of a statistical model. In the case of a language model, the parameters are the probabilities of observing a particular word given a sequence of preceding words. This can be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta) = \\prod_{i=1}^{T} P(w_i | w_{i-N+1}, ..., w_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Here, $\\theta$ represents the set of parameters that we want to estimate. These parameters are the conditional probabilities of a word given its (n-1)-word context, i.e., $P(w_i | w_{i-1}, ... w_{i-N+1}$ for all possible combinations of words in the context. $T$ is the length of the training data and $N$ is the window of the N-gram.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\theta} = \\arg\\max_{\\theta} \\prod_{i=1}^{T} P(w_i | w_{i-N+1}, ..., w_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "The objective of MLE is find the values of $\\theta$ that maximize the likelihood of the observed data.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{MLE} = \\arg\\max_{\\theta} \\prod_{i=1}^{N} P(w_i | w_{i-N+1}, ..., w_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "For computational simplicity, we often work with the log-likelihood instead of the likelihood, as the product of probabilities can become very small and cause numerical instability. The log-likelihood is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{MLE} = \\arg\\max_{\\theta} \\sum_{i=1}^{N} \\log P(w_i | w_{i-N+1}, ..., w_{i-1}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "In practice, the MLE parameters can be estimated using the relative frequencies of the n-grams in the training data.\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{MLE} = P(w_i | w_{i-N+1}, ..., w_{i-1}) &= \\frac{C(w_{i-N+1},...,w_{i})}{\\sum_{w \\in \\mathcal{V}} C(w_{i-N+1},...,w_{i-1}, w)} \\\\\n",
    "&= \\frac{C(w_{i-N+1}, ..., w_i)}{C(w_{i-N+1}, ..., w_{i-1})}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Unigram Model MLE:\n",
    "\n",
    "\\begin{align}\n",
    "p(w_i) = \\frac{C(w_i)}{\\sum_{w \\in \\mathcal{V}} C(w)}\n",
    "\\end{align}\n",
    "\n",
    "In this expression, $p(w_i)$ is the probability of observing the word $w_i$, $C(w_i)$ is the count of occurrences of the word $w_i$ in the corpus, and $\\sum_w C(w)$ is the total count of words in the corpus.\n",
    "\n",
    "#### Bigram Model MLE\n",
    "\n",
    "\\begin{align}\n",
    "p(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{\\sum_{w \\in \\mathcal{V}} C(w_{i-1}, w)}\n",
    "\\end{align}\n",
    "\n",
    "In this expression, $p(w_i | w_{i-1})$ is the probability of observing the word $w_i$ given the previous word $w_{i-1}$, $C(w_{i-1}, w_i)$ is the count of occurrences of the bigram $(w_{i-1}, w_i)$ in the corpus, and $\\sum_{w} C(w_{i-1}, w)$ is the total count of bigrams that start with the word $w_{i-1}$.\n",
    "\n",
    "\n",
    "#### Trigram Model MLE\n",
    "\\begin{align}\n",
    "p(w_i | w_{i-1}, w_{i-2}) = \\frac{C(w_{i-2}, w_{i-1}, w_i)}{\\sum_{w \\in \\mathcal{V}} C(w_{i-2}, w_{i-1}, w)}\n",
    "\\end{align}\n",
    "\n",
    "In this expression, $p(w_i | w_{i-1}, w_{i-2})$ is the probability of observing the word $w_i$ given the two previous words $w_{i-1}$ and $w_{i-2}$, $C(w_{i-2}, w_{i-1}, w_i)$ is the count of occurrences of the trigram $(w_{i-2}, w_{i-1}, w_i)$ in the corpus, and $\\sum_{w'} C(w_{i-2}, w_{i-1}, w)$ is the total count of trigrams that start with the sequence of words $(w_{i-2}, w_{i-1})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791cc9f-6408-4a3d-be74-805fcd7b887c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Proof MLE for Bigram\n",
    "\n",
    "For any given sequence of words $\\textbf{w}$ of $|\\textbf{w}| = n$ e.g $w = \\langle w_1, w_2, w_1, w_5, w_7, w_2\\rangle$. It follows:\n",
    "\n",
    "\\begin{align}\n",
    "p(w) = \\prod\\limits_{i=1}^{n}p(w_i)^{s(w_i)}\\prod\\limits_{i=1}^{n-1}\\prod\\limits_{j=1}^{n}p(w_j|w_i)^{C(w_i, w_j)}\n",
    "\\end{align}\n",
    "\n",
    "where $c(w_i, w_j)$ is the count of word sequence $w_i, w_j$ in the sentence and\n",
    "\n",
    "$$\n",
    "s(w_i) = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if } w_i \\text{ is the first word} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Taking the log of $p(w)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p(w) &= \\sum\\limits_{i=1}^{n} s(w_i) \\log p(w_i) + \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} c(w_i, w_j) \\log p(w_j|w_i) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To maximize $p(w)$, equivalently we have the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\max \\log p(w), \\forall i \\in {1, \\ldots, n} subject to \\sum\\limits_{j=1}^{n} p(w_j|w_i) = 1\n",
    "\\end{align}\n",
    "\n",
    "Equivalently, we introduce auxiliary optimization function using Lagrange multiplier ($\\sum\\limits_{j=1}^{n} p(w_j|w_i) - 1 = 0$):\n",
    "\n",
    "$$\n",
    "L = \\sum\\limits_{i=1}^{n} s(w_i) \\log p(w_i) + \\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n} c(w_i, w_j) \\log p(w_j|w_i) + \\sum\\limits_{i=1}^{n} \\lambda_i \\left(\\sum\\limits_{j=1}^{n} p(w_j|w_i) - 1\\right)\n",
    "$$\n",
    "\n",
    "For any $p(w_k|w_i)$, we take the derivatives of $L$ respective to $p(w_k|w_i)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial p(w_k|w_i)} &= c(w_i, w_k)\\frac{1}{p(w_k|w_i)} + \\lambda_i = 0 \\\\\n",
    "p(w_k|w_i) &= -\\frac{c(w_i, w_k)}{\\lambda_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because $\\sum\\limits_{j=1}^{n} p$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\sum\\limits_{j=1}^{n} p(w_j|w_i) &= \\sum\\limits_{j=1}^{n} -\\frac{c(w_i, w_j)}{\\lambda_i} \\\\\n",
    "&= -\\frac{1}{\\lambda_i}\\sum\\limits_{j=1}^{n} c(w_i, w_j) \\\\\n",
    "&= -\\frac{1}{\\lambda_i}\\left(\\sum\\limits_{j=1}^{n} c(w_i, w_j)\\right) \\\\\n",
    "&= -\\frac{1}{\\lambda_i} \\\\\n",
    "\\lambda_i &= -\\sum\\limits_{j=1}^{n} c(w_i, w_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Because $p(w_k|w_i) = \\frac{c(w_i, w_k)}{\\sum\\limits_{j=1}^{n} c(w_i, w_j)}$, therefore\n",
    "\n",
    "$$\n",
    "p(w_k|w_i) = \\frac{c(w_i, w_k)}{\\sum\\limits_{j=1}^{n} c(w_i, w_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d98ca-aead-4a10-b2dd-1ae0b89554e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building a Character Level Bigram Language Model with Pytorch\n",
    "\n",
    "\n",
    "### Introduction\n",
    "In this tutorial we will be creating our own bigram language model to perform next character prediction. Our goal is to primarily get familiar\n",
    "with Pytorch by using the library to manipulate tensors. \n",
    "\n",
    "We will demonstrate how this simple language model can be used to generate names, albeit, rather poorly but hopefully provide foundational\n",
    "knowledge language models that will extend to more powerful models in the future.\n",
    "\n",
    "Note this tutorial is largely inspired by Andrej Karpathy's makemore[!https://www.youtube.com/@AndrejKarpathy] videos, which i suggest you checkout\n",
    "if you have additional time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143099f-8394-42be-b586-9ba9274277a9",
   "metadata": {},
   "source": [
    "#### How can building a language model be used for next character prediction ?\n",
    "\n",
    "We'll rewrite some of our notation from above to build some more intuition about the next sequence prediction task. \n",
    "\n",
    "Let's represent an input sequence $\\textbf{x}$ and an output sequence $\\textbf{y}$ both as sequence of characters $c_1c_2c_3...,c_n$, where $c_i$ is a letter of the english alphabet ${\\mathcal{C}}$. Our goal is to learn a function $f(x) = \\hat{y}$ such that:  \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = \\arg \\max_{y \\in \\mathcal{Y}} P(\\textbf{y} | \\textbf{x})\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{Y}$ is the set of all possible sequences of characters and $\\hat{y}$ is the predicted output sequence of characters.\n",
    "\n",
    "The expression represents the idea that we want to find the output sequence $\\hat{y}$ that maximizes the probability of observing the true output sequence $y$ given the input prefix sequence $x$. This is a common approach to language modeling, where we model the probability distribution of output sequences given input sequences, and use this model to predict the most likely output sequence for a given input sequence.\n",
    "\n",
    "We know from earlier that there are multiple ways to model $P(\\textbf{y} | \\textbf{x})$. Using a bigram model this would like:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "P(\\textbf{y} | \\textbf{x}) &= \\prod_{i=1}^n P(c_i|c_{i-1} ... c_{i-n}) \\\\ \n",
    "&\\approx \\prod_{i=1}^n P(c_i|c_{i-1}) \\\\\n",
    "&= \\frac{C(c_i)}{C(c_{i-1})}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea5ab0-f539-4b2a-867a-86ce6d34b047",
   "metadata": {},
   "source": [
    "#### Step 1: Reading the Data\n",
    "\n",
    "\n",
    "Let's start by reading in our dataset which comprises of a list of `32033` names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d3028-af18-463f-ba95-a42a69620614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import read_file #helper function for reading in files.\n",
    "\n",
    "\n",
    "names = list(read_file('names.txt'))\n",
    "\n",
    "print(f'Total number of names: {len(names)}')\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96048d2d-169d-4f47-9f05-213c69d03fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 2: Preprocessing our data\n",
    "\n",
    "- Create bigram pairs.\n",
    "- Add Start and End Tokens\n",
    "- Create dictionary of bigram counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17e9ff-1593-465a-986f-39006140a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in names:\n",
    "    n = ['<S>'] + list(n) + ['<E>']\n",
    "    for c1, c2 in zip(n, n[1:]):\n",
    "        b[(c1,c2)] = b.get((c1,c2),0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff77b0-ff0e-468a-baee-1647cb883975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 3: Create bigram torch tensors\n",
    "- Create torch tensor. (Look at dtype)\n",
    "- Create count matrix.\n",
    "- Create ctoi mapping\n",
    "- Create itos Mapping\n",
    "- Plot bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946fddf-60d7-433e-94f8-8d168c9b6c04",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = torch.zeros((28,28), dtype=torch.int32)\n",
    "C = sorted(list(set(''.join(names))))\n",
    "ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "ctoi['<S>'] = 0\n",
    "ctoi['<E>'] = 27\n",
    "sorted(ctoi.items(), key = lambda v: v[1])\n",
    "for n in names:\n",
    "    n = ['<S>'] + list(n) +['<E>']\n",
    "    for c1, c2 in zip(n, n[1:]):\n",
    "        b[(c1,c2)] = b.get((c1,c2),0) + 1 \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "from utilities import plot_char_matrix\n",
    "\n",
    "plot_char_matrix(ctoi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2a13d54a-8483-4faf-8d7f-da3eed14fae7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = torch.zeros((28,28), dtype=torch.int32)\n",
    "C = sorted(list(set(''.join(names))))\n",
    "ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "ctoi['<S>'] = 0\n",
    "ctoi['<E>'] = 27\n",
    "sorted(ctoi.items(), key = lambda v: v[1])\n",
    "for n in names:\n",
    "    n = ['<S>'] + list(n) +['<E>']\n",
    "    for c1, c2 in zip(n, n[1:]):\n",
    "        b[(c1,c2)] = b.get((c1,c2),0) + 1 \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d472e-fda8-4693-9e4e-412bc92e595e",
   "metadata": {},
   "source": [
    "#### Step 4: Generate Samples\n",
    "- Convert row to probabilities\n",
    "- Create multinomial (https://pytorch.org/docs/stable/generated/torch.multinomial.html)\n",
    "- Demonstrate how torch.multinomial works.\n",
    "- Sample from distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7efa5be6-d91f-4deb-9e34-e6755b9e2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "380b6773-65d7-446a-9b64-4f05ed672557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  4,  9, 11, 19,  1,  6,  1, 14,  3, 13, 19, 11,  1,  4, 11, 10, 14,\n",
       "         4, 10])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(p, num_samples=20, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0124ef16-028e-4567-bf52-29934c0b2979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "25\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "P = N.float()\n",
    "P = P / P.sum(1, keepdims=True)\n",
    "ix = 0\n",
    "while True:\n",
    "    p = P[ix].float()\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "    print(ix)\n",
    "    if ix == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "45240942-05dc-4995-8475-9ba8ff8c4d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3643199539.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[195], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    - Explain pytorch sum function (https://pytorch.org/docs/stable/generated/torch.sum.html)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#### Step 5: Efficient Computation of P and Broadcasting rules.\n",
    "- Explain pytorch sum function (https://pytorch.org/docs/stable/generated/torch.sum.html)\n",
    "- Pytorch Broadcasting Semantics (https://pytorch.org/docs/stable/notes/broadcasting.html)\n",
    "- Rewrite the count statistics instead as probability statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837b2b1-af91-4e1a-93fa-c3636aeeb78e",
   "metadata": {},
   "source": [
    "#### General semantics\n",
    "\n",
    "Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "Each tensor has at least one dimension.\n",
    "\n",
    "When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4caba99e-cf9c-44fd-8373-e6aa16e6f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = N.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e7cd30f2-b68b-43cc-9f02-a46422c01853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27, 27\n",
    "#  1, 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ca8344f7-d856-48a4-9d56-34e9fcecdb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5220b494-ba11-49e9-95b3-ba45a66fd43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of .e: 0.0478\n",
      "Probability of em: 0.0377\n",
      "Probability of mm: 0.0253\n",
      "Probability of ma: 0.3899\n",
      "Probability of a.: 0.0000\n",
      "Probability of .o: 0.0123\n",
      "Probability of ol: 0.0780\n",
      "Probability of li: 0.1777\n",
      "Probability of iv: 0.0152\n",
      "Probability of vi: 0.3541\n",
      "Probability of ia: 0.1381\n",
      "Probability of a.: 0.0000\n"
     ]
    }
   ],
   "source": [
    "P = N.float()\n",
    "P = P / P.sum(1, keepdims=True)\n",
    "C = sorted(list(set(''.join(names))))\n",
    "ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "ctoi['.'] = 0\n",
    "sorted(ctoi.items(), key = lambda v: v[1])\n",
    "for n in names[:2]:\n",
    "    n = ['.'] + list(n) +['.']\n",
    "    for c1, c2 in zip(n, n[1:]): \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        prob = P[ix1, ix2]\n",
    "        print(f\"Probability of {c1}{c2}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "387a194d-b6a2-4b9d-bd9b-f6b13e2ca924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 6: Assessing the quality of the model\n",
    "- Probabilities should be as close to one as possible.\n",
    "- Ergo. the product of all the probabilities should be = 1 for a perfect model.\n",
    "- Products are messy to work with so we take logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7d33ac6d-88a0-490f-99ce-28a4af516924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of .o: 0.0123 -4.3982\n",
      "Probability of o.: 0.0000 -inf\n",
      "Probability of .l: 0.0491 -3.0144\n",
      "Probability of l.: 0.0000 -inf\n",
      "Probability of .i: 0.0184 -3.9927\n",
      "Probability of i.: 0.0000 -inf\n",
      "Probability of .v: 0.0117 -4.4449\n",
      "Probability of v.: 0.0000 -inf\n",
      "Probability of .i: 0.0184 -3.9927\n",
      "Probability of i.: 0.0000 -inf\n",
      "Probability of .a: 0.1377 -1.9829\n",
      "Probability of a.: 0.0000 -inf\n"
     ]
    }
   ],
   "source": [
    "#P = N.float()\n",
    "#P = P / P.sum(1, keepdims=True)\n",
    "#C = sorted(list(set(''.join(names))))\n",
    "#ctoi = {c:i+1 for i,c in enumerate(C)}\n",
    "#ctoi['.'] = 0\n",
    "#sorted(ctoi.items(), key = lambda v: v[1])\n",
    "\n",
    "\n",
    "\n",
    "for n in names[1]:\n",
    "    n = ['.'] + list(n) +['.']\n",
    "    for c1, c2 in zip(n, n[1:]): \n",
    "        ix1, ix2 = ctoi[c1], ctoi[c2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = prob.log()\n",
    "        print(f\"Probability of {c1}{c2}: {prob:.4f} {logprob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa796d3-9743-4e72-82ec-c59d8c6dbf09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
